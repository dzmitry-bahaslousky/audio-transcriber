# Dockerfile for CUDA/GPU deployment
# Optimized for production inference with NVIDIA GPUs
# Build with: docker build -f Dockerfile.cuda -t audio-transcriber:cuda .

ARG PYTHON_VERSION=3.11
ARG CUDA_VERSION=12.1.1
ARG CUDNN_VERSION=8

# =============================================================================
# Stage 1: Dependencies builder
# =============================================================================
FROM nvidia/cuda:${CUDA_VERSION}-cudnn${CUDNN_VERSION}-runtime-ubuntu22.04 AS builder

# Install Python and build dependencies
ARG PYTHON_VERSION
RUN apt-get update && apt-get install -y --no-install-recommends \
    python${PYTHON_VERSION} \
    python3-pip \
    python3-dev \
    build-essential \
    git \
    ffmpeg \
    libsndfile1 \
    && ln -sf /usr/bin/python${PYTHON_VERSION} /usr/bin/python \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /code

# Copy requirements files
COPY requirements.txt requirements-cuda.txt ./

# Create virtual environment for better isolation
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Upgrade pip and install build tools
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# Install CUDA-enabled PyTorch and dependencies
RUN pip install --no-cache-dir -r requirements-cuda.txt && \
    pip install --no-cache-dir -r requirements.txt

# =============================================================================
# Stage 2: Runtime image
# =============================================================================
FROM nvidia/cuda:${CUDA_VERSION}-cudnn${CUDNN_VERSION}-runtime-ubuntu22.04 AS runtime

# Install Python and runtime dependencies
ARG PYTHON_VERSION
RUN apt-get update && apt-get install -y --no-install-recommends \
    python${PYTHON_VERSION} \
    python3-pip \
    ffmpeg \
    libsndfile1 \
    && ln -sf /usr/bin/python${PYTHON_VERSION} /usr/bin/python \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user for security
RUN useradd -m -u 1000 -s /bin/bash appuser

# Set working directory
WORKDIR /code

# Copy virtual environment from builder
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy application code
COPY --chown=appuser:appuser ./app /code/app

# Create cache directory for model downloads with proper permissions
RUN mkdir -p /home/appuser/.cache/whisper && \
    chown -R appuser:appuser /home/appuser/.cache

# Switch to non-root user
USER appuser

# Expose port
EXPOSE 8000

# Health check with longer start period for GPU model loading
HEALTHCHECK --interval=30s --timeout=10s --start-period=90s --retries=3 \
    CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:8000/docs').read()" || exit 1

# Run the FastAPI application with uvicorn
# Using --host 0.0.0.0 to accept connections from outside the container
# Using --workers 1 to avoid model loading in multiple processes (WhisperX singleton)
CMD ["uvicorn", "app.main:api", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
